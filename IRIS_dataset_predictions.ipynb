{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score, KFold, train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "features = iris.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=111)\n",
    "crossvalidation = KFold(n=X_train.shape[0], n_folds=5,shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization:1.0  Accuracy: 0.942\n",
      "Regularization:1.5  Accuracy: 0.958\n",
      "Regularization:2.0  Accuracy: 0.967\n",
      "Regularization:2.5  Accuracy: 0.958\n",
      "Regularization:3.0  Accuracy: 0.958\n",
      "Regularization:3.5  Accuracy: 0.958\n",
      "Regularization:4.0  Accuracy: 0.958\n",
      "Regularization:4.5  Accuracy: 0.958\n",
      "Regularization:5.0  Accuracy: 0.958\n",
      "Regularization:5.5  Accuracy: 0.958\n",
      "Regularization:6.0  Accuracy: 0.958\n",
      "Regularization:6.5  Accuracy: 0.958\n",
      "Regularization:7.0  Accuracy: 0.958\n",
      "Regularization:7.5  Accuracy: 0.958\n",
      "Regularization:8.0  Accuracy: 0.958\n",
      "Regularization:8.5  Accuracy: 0.958\n",
      "Regularization:9.0  Accuracy: 0.958\n",
      "Regularization:9.5  Accuracy: 0.958\n",
      "[0 0 2 2 1 0 0 2 2 1 2 0 1 2 2 0 2 1 0 2 1 2 1 1 2 0 0 2 0 2]\n",
      "[0 0 2 2 1 0 0 2 2 1 2 0 2 2 2 0 2 1 0 2 1 2 1 1 2 0 0 1 0 2]\n",
      "0.933333333333\n",
      "2.0\n",
      "[0 0 2 2 1 0 0 2 2 1 2 0 2 2 2 0 2 1 0 2 1 2 1 1 2 0 0 2 0 2]\n",
      "[0 0 2 2 1 0 0 2 2 1 2 0 2 2 2 0 2 1 0 2 1 2 1 1 2 0 0 1 0 2]\n",
      "0.966666666667\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn import linear_model\n",
    "maxScore = 0.0;\n",
    "maxReg = 0.0;\n",
    "\n",
    "for regularization in np.arange(1, 10, 0.5):\n",
    "\tlogClassifier = linear_model.LogisticRegression(C=regularization,random_state=111)\n",
    "\t\n",
    "\tscore = np.mean(cross_val_score(logClassifier, X_train, y_train, scoring='accuracy', cv=crossvalidation, n_jobs=1))\n",
    "\tif(score > maxScore):\n",
    "\t\t\tmaxScore = score\n",
    "\t\t\tmaxReg= regularization\t\n",
    "\tprint 'Regularization:{0}  Accuracy: {1:.3f}'.format(regularization,score) #to identify the best regularization iteratively\n",
    "\n",
    "#taking C = 150 as regularization factor\n",
    "logClassifier = linear_model.LogisticRegression(C=150,random_state=111)\n",
    "logClassifier.fit(X_train, y_train)\n",
    "predicted = logClassifier.predict(X_test)\n",
    "print predicted\n",
    "print y_test\n",
    "print accuracy_score(y_test, predicted)  # 1.0 is 100 percent accuracy\n",
    "\n",
    "print maxReg\n",
    "\n",
    "#taking value found in loop above as regularization factor\n",
    "logClassifier = linear_model.LogisticRegression(C=maxReg,random_state=111)\n",
    "logClassifier.fit(X_train, y_train)\n",
    "predicted = logClassifier.predict(X_test)\n",
    "print predicted\n",
    "print y_test\n",
    "print accuracy_score(y_test, predicted)  # 1.0 is 100 percent accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators:10  Accuracy: 0.967\n",
      "n_estimators:20  Accuracy: 0.958\n",
      "n_estimators:30  Accuracy: 0.967\n",
      "n_estimators:40  Accuracy: 0.967\n",
      "n_estimators:50  Accuracy: 0.967\n",
      "n_estimators:60  Accuracy: 0.967\n",
      "n_estimators:70  Accuracy: 0.975\n",
      "n_estimators:80  Accuracy: 0.967\n",
      "n_estimators:90  Accuracy: 0.967\n",
      "n_estimators:100  Accuracy: 0.967\n",
      "0.9\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "maxScore = 0.0;\n",
    "maxTreeSet = 0;\n",
    "\n",
    "for treeset in np.arange(10, 110, 10):\n",
    "\tclf = RandomForestClassifier(n_estimators=treeset)\n",
    "\t\n",
    "\tscore = np.mean(cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=crossvalidation, n_jobs=1))\n",
    "\tif(score > maxScore):\n",
    "\t\t\tmaxScore = score\n",
    "\t\t\tmaxTreeSet= treeset\t\n",
    "\tprint 'n_estimators:{0}  Accuracy: {1:.3f}'.format(treeset,score) #to identify the best depth iteratively\n",
    "\n",
    "\n",
    "## Building the model using loop max\n",
    "clf = RandomForestClassifier(n_estimators=maxTreeSet)\n",
    "\n",
    "## Training the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "## Predicting the Species\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "## Checking the accuracy\n",
    "print(accuracy_score(predicted, y_test))\n",
    "\n",
    "## Building the model using 100 trees\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "## Training the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "## Predicting the Species\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "## Checking the accuracy\n",
    "print(accuracy_score(predicted, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth:1  Accuracy: 0.617\n",
      "Depth:2  Accuracy: 0.975\n",
      "Depth:3  Accuracy: 0.992\n",
      "Depth:4  Accuracy: 0.983\n",
      "[0 0 2 2 2 0 0 2 2 1 2 0 1 2 2 0 2 1 0 2 1 2 1 1 2 0 0 2 0 2]\n",
      "[0 0 2 2 1 0 0 2 2 1 2 0 2 2 2 0 2 1 0 2 1 2 1 1 2 0 0 1 0 2]\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "#CART\n",
    "from sklearn import tree\n",
    "maxScore = 0.0;\n",
    "maxDepth = 0;\n",
    "\n",
    "for depth in range(1,10):\n",
    "\ttree_classifier = tree.DecisionTreeClassifier(max_depth=depth, random_state=0)\n",
    "\tif tree_classifier.fit(X_train, y_train).tree_.max_depth < depth:\n",
    "\t\tbreak\n",
    "\tscore = np.mean(cross_val_score(tree_classifier, X_train, y_train, scoring='accuracy', cv=crossvalidation, n_jobs=1))\n",
    "\tif(score > maxScore):\n",
    "\t\t\tmaxScore = score\n",
    "\t\t\tmaxDepth = depth\t\n",
    "\tprint 'Depth:{0}  Accuracy: {1:.3f}'.format(depth,score) #to identify the best depth iteratively\n",
    "\n",
    "tree_classifier = tree.DecisionTreeClassifier(max_depth=maxDepth, random_state=0)\n",
    "tree_classifier.fit(X_train,y_train)\n",
    "\n",
    "predicted = tree_classifier.predict(X_test)\n",
    "\n",
    "print predicted\n",
    "print y_test\n",
    "\n",
    "## Checking the accuracy\n",
    "print(accuracy_score(predicted, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_sizes:1  Accuracy: 0.250\n",
      "hidden_layer_sizes:2  Accuracy: 0.733\n",
      "hidden_layer_sizes:3  Accuracy: 0.250\n",
      "hidden_layer_sizes:4  Accuracy: 0.942\n",
      "hidden_layer_sizes:5  Accuracy: 0.425\n",
      "hidden_layer_sizes:6  Accuracy: 0.250\n",
      "hidden_layer_sizes:7  Accuracy: 0.933\n",
      "hidden_layer_sizes:8  Accuracy: 0.967\n",
      "hidden_layer_sizes:9  Accuracy: 0.975\n",
      "hidden_layer_sizes:10  Accuracy: 0.983\n",
      "hidden_layer_sizes:11  Accuracy: 0.975\n",
      "hidden_layer_sizes:12  Accuracy: 0.967\n",
      "hidden_layer_sizes:13  Accuracy: 0.975\n",
      "hidden_layer_sizes:14  Accuracy: 0.983\n",
      "0.966666666667\n",
      "0.933333333333\n"
     ]
    }
   ],
   "source": [
    "#Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "maxSize = 0;\n",
    "maxScore = 0.0;\n",
    "\n",
    "for size in range(1,15):\n",
    "\tclf = MLPClassifier(solver='lbfgs', alpha=1e-5,  hidden_layer_sizes=(size,), random_state=1)\n",
    "\t\n",
    "\tscore = np.mean(cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=crossvalidation, n_jobs=1))\n",
    "\tif(score > maxScore):\n",
    "\t\t\tmaxScore = score\n",
    "\t\t\tmaxSize = size\t\n",
    "\tprint 'hidden_layer_sizes:{0}  Accuracy: {1:.3f}'.format(size,score) #to identify the best hidden_layer_sizes iteratively\n",
    "#Using value found above\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,  hidden_layer_sizes=(maxSize,), random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "## Checking the accuracy\n",
    "print(accuracy_score(predicted, y_test))\n",
    "\n",
    "#Using fixed size of 15 just to see the improvement above\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,  hidden_layer_sizes=(15,), random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "## Checking the accuracy\n",
    "print(accuracy_score(predicted, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report:\n",
    "\n",
    "For each of the algorithms we have used KFold with fold = 5 for cross validation.\n",
    "\n",
    "    1. Logistic Regression:\n",
    "    \n",
    "       We can observe from above that the regularization factor is used to find a better accuracy. We went in  a loop for different values of regularization and chose the best. It is quite surprising to note that the higher value of regularization factor C = 150, gives a lesser value of accuracy.\n",
    "       The lesser value of C = 1, gives a higher accuracy.\n",
    "       \n",
    "    2. Random Forest:\n",
    "        \n",
    "       For Random Forest we used the forest size as a parameter for better fitting the data. The same as above a loop was used to find the best forest size. Higher forest size has given better accuracy. There may be some tree sizes which may not contibute much to the accuracy. But, the ones that do makes a huge difference and that is the reason we go for higher values of tree sizes in a random forest.\n",
    "    \n",
    "    3. CART:\n",
    "    \n",
    "       Using CART, we followed the same procedures above. This time the depth of the tree is the parameter that we used to choose a better model. The more the depth the more complex the tree becomes. It is also interesting to note that after a point of depth the accuracy starts decreasing. So we need to find the depth with the best score. In our example above we can see that depth = 3 gives the best results. Depth = 4 has a lower result than Depth =3.\n",
    "       \n",
    "    4. Neural Network:\n",
    "    \n",
    "      Using Neural networks we used MLPClassifier from scikit learn. Here the alpha and the hidden layer sizes were the parameters we used to find the best model. We can observe that alpha 1e-5 and hidden layer size lesser of 14 gives the most accurate result.\n",
    "    \n",
    "    Neural Networks is the best model here in our opinion. It gave a good accuracy.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
